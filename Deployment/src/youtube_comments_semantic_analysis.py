# -*- coding: utf-8 -*-
"""YouTube_Comments_Semantic_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fo0Fi5hXFHUXJ13ykwrgd37rzOEtXv3E
"""

!pip install joblib -q
!pip install pyngrok -q
!pip install mlflow -q
!pip install scikit-learn --quiet

from google.colab import drive
drive.mount('/content/drive')

import os
import re
import numpy as np
import pandas as pd
import torch
import joblib
import mlflow.sklearn
from sklearn.naive_bayes import MultinomialNB
from mlflow.models import infer_signature
from pyngrok import ngrok, conf
from google.colab import userdata
import subprocess
import time

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Text Preprocessing
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Scikit-learn for ML Model
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler

authtoken = userdata.get('ngork_auth')
conf.get_default().authtoken = authtoken

MLFLOW_DATA_PATH = "/content/drive/MyDrive/mlflow_data"
MLFLOW_DB_PATH = os.path.join(MLFLOW_DATA_PATH, "mlflow2.db")
MLFLOW_ARTIFACT_PATH = os.path.join(MLFLOW_DATA_PATH, "artifacts2")

TRACKING_URI = f"sqlite:///{MLFLOW_DB_PATH}"
mlflow.set_tracking_uri(TRACKING_URI)

command = (
    f"mlflow server "
    f"--backend-store-uri sqlite:///{MLFLOW_DB_PATH} "
    f"--default-artifact-root {MLFLOW_ARTIFACT_PATH} "
    f"--host 0.0.0.0 "
    f"--port 5000"
)

!pkill -f "mlflow"
ngrok.kill()

get_ipython().system_raw(f"{command} &")

nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

def set_style():
    """Sets a consistent plotting style."""
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (10, 6)
    plt.rcParams['font.size'] = 12

def plot_confusion_matrix(y_true, y_pred, model_name, class_names):
    """Plots a confusion matrix using seaborn."""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

def evaluate_model(y_true, y_pred, model_name, class_names):
    """Prints classification report and plots confusion matrix."""
    print(f"--- Evaluation Metrics for {model_name} ---")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print("\nClassification Report:")

    print(classification_report(y_true, y_pred, target_names=class_names))
    plot_confusion_matrix(y_true, y_pred, model_name, class_names)

set_style()

from google.colab import files

print("Please upload your kaggle.json file")
uploaded = files.upload()

# Checking the file
if 'kaggle.json' in uploaded:
    print("\n'kaggle.json' uploaded successfully!")
else:
    print("\n'kaggle.json' not found. Please re-run the cell and upload the file.")
    raise SystemExit

# Create the .kaggle directory
!mkdir -p ~/.kaggle

# Move the uploaded kaggle.json to the new directory
!mv kaggle.json ~/.kaggle/

# Set the correct file permissions (read/write for owner only) to avoid warnings
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list -s "youtube_comments_cleaned.csv"

!kaggle datasets download -d "amaanpoonawala/youtube-comments-sentiment-dataset"

ls -ltr

!unzip -oq {"youtube-comments-sentiment-dataset.zip"}

df=pd.read_csv('youtube_comments_cleaned.csv')

print("\n--- First 5 Rows ---")
print(df.head())

# Checking data types and null values
print("\n--- Data Info ---")
df.info()

# Checking for duplicates
print(f"\nNumber of duplicate rows: {df.duplicated().sum()}")
df.drop_duplicates(inplace=True)
print(f"Shape after dropping duplicates: {df.shape}")

print(df.isnull().sum())

df.dropna(subset=['CommentText'], inplace=True)
df['CommentText'] = df['CommentText'].astype(str)

plt.figure(figsize=(8, 5))
sns.countplot(x='Sentiment', data=df, order=df['Sentiment'].value_counts().index)
plt.title('Distribution of Sentiments (in Sampled Data)')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()
print("Distribution in the sample:")
print(df['Sentiment'].value_counts(normalize=True))

df['comment_length'] = df['CommentText'].astype(str).apply(len)
df['word_count'] = df['CommentText'].astype(str).apply(lambda x: len(x.split()))

plt.figure(figsize=(12, 5))
 plt.subplot(1, 2, 1)
 sns.histplot(data=df, x='comment_length', hue='Sentiment', kde=True, bins=50)
 plt.title('Distribution of Comment Length (Characters)')
 plt.xlim(0, 500)

plt.subplot(1, 2, 2)
sns.histplot(data=df, x='word_count', hue='Sentiment', kde=True, bins=50)
plt.title('Distribution of Word Count')
plt.xlim(0, 100)
plt.tight_layout()
plt.show()

df['log_likes'] = np.log1p(df['Likes'])

# Feature Engineering
df['has_replies'] = (df['Replies'] > 0).astype(int)
# We will use CategoryID as a categorical feature later in the pipeline
print("Engineered features 'log_likes' and 'has_replies' created.")

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
        text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A).lower()
        tokens = word_tokenize(text)
        cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]
        return " ".join(cleaned_tokens)

df['cleaned_comment'] = df['CommentText'].apply(preprocess_text)
print("Text cleaning complete.")

sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}
df['label'] = df['Sentiment'].map(sentiment_map)
CLASS_NAMES = list(sentiment_map.keys())
print("Label encoding complete.")
print(CLASS_NAMES)

# Selecting the features we want to use
features = ['cleaned_comment', 'log_likes', 'has_replies', 'CategoryID']
X = df[features]
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42, stratify=y)
print(f"\nTraining data shape: {X_train.shape}, Testing data shape: {X_test.shape}")

print(f"Length of X_train: {len(X_train)}")
print(f"Length of y_train: {len(y_train)}")
print(f"Type of X_train: {type(X_train)}")
print(f"Type of y_train: {type(y_train)}")

preprocessor = ColumnTransformer(
transformers=[
            # Transformer 1: Applying TF-IDF to the 'cleaned_comment' column
            ('tfidf', TfidfVectorizer(max_features=7000, ngram_range=(1, 2)), 'cleaned_comment'),

            # Transformer 2: Scaling our numerical features
            ('numeric', StandardScaler(), ['log_likes', 'has_replies']),

            # Transformer 3: One-Hot Encode the 'CategoryID'
            ('categorical', OneHotEncoder(handle_unknown='ignore'), ['CategoryID'])
        ],
        remainder='drop' # Droping any columns we didn't specify
    )

pipeline_lr_engineered = Pipeline([
        ('preprocessor', preprocessor),
        ('clf', LogisticRegression(solver='liblinear', random_state=42, max_iter=1000))
    ])

pipeline_lr_engineered.fit(X_train, y_train)

    # Predictions
y_pred_lr_engineered = pipeline_lr_engineered.predict(X_test)

evaluate_model(y_true=y_test, y_pred=y_pred_lr_engineered, model_name="Logistic Regression with Engineered Features", class_names=CLASS_NAMES)

# for key in sorted(params.keys()):
#         print(key)

with mlflow.start_run(run_name="Logistic_Regression_With_Features") as run:
    print("\n--- Starting MLflow Run ---")
    run_id = run.info.run_id
    print(f"MLflow Run ID: {run_id}")

    # a) Log Parameters from the correct pipeline
    params = pipeline_lr_engineered.get_params()
    mlflow.log_param("model_type", "LogisticRegression")
    # Correctly access nested parameters from the ColumnTransformer
    mlflow.log_param("tfidf_max_features", params['preprocessor__tfidf__max_features'])
    mlflow.log_param("tfidf_ngram_range", params['preprocessor__tfidf__ngram_range'])
    print("Logged Parameters.")

    # b) Log Metrics using the pre-calculated predictions
    accuracy = accuracy_score(y_test, y_pred_lr_engineered)
    mlflow.log_metric("accuracy", accuracy)
    print(f"Logged Accuracy: {accuracy:.4f}")

    # c) Log the correct Model Itself
    print("Logging the scikit-learn pipeline to MLflow...")
    mlflow.sklearn.log_model(
        sk_model=pipeline_lr_engineered,
        artifact_path="sentiment-model",
        registered_model_name="logistic-regression-engineered-features"
    )

import sqlite3
db_path = "/content/drive/MyDrive/mlflow_data/mlflow2.db"

try:
    conn = sqlite3.connect(db_path)

    print(f"Successfully connected to the database at: {db_path}")

    print("\n--- Tables in the database ---")
    tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn)
    print(tables)



    print("\n--- Content of 'experiments' table ---")
    experiments_df = pd.read_sql_query("SELECT * FROM experiments", conn)
    print(experiments_df[['experiment_id', 'name', 'lifecycle_stage']])

    print("\n--- Most recent 5 runs in the 'runs' table ---")
    runs_df = pd.read_sql_query("SELECT * FROM runs ORDER BY start_time DESC LIMIT 5", conn)
    # Display the most relevant columns
    print(runs_df[['run_uuid', 'experiment_id', 'name', 'status', 'lifecycle_stage']])


    conn.close()

except Exception as e:
  print(f"❌ ERROR: Could not read the database file.")
  print(f"Please check if the path '{db_path}' is correct and the file exists.")
  print(f"Original error: {e}")

stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

def preprocess_for_nb(text):
    if not isinstance(text, str):
        return ""
    # Converting to lowercase and remove non-alphabetic characters
    text = re.sub(r'[^a-zA-Z\s]', '', text, re.I|re.A).lower()
    # Tokenizing the text
    tokens = word_tokenize(text)
    #Removing stop words and apply stemming
    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and len(word) > 2]
    return " ".join(stemmed_tokens)

print("\n--- Preprocessing data... ---")
df.dropna(subset=['CommentText'], inplace=True)
df['cleaned_comment'] = df['CommentText'].apply(preprocess_for_nb)
print("Preprocessing complete.")

print(f"Length of X_train: {len(X_train)}")
print(f"Length of y_train: {len(y_train)}")
print(f"Type of X_train: {type(X_train)}")
print(f"Type of y_train: {type(y_train)}")

# Define the different types of columns
text_feature = 'cleaned_comment'
numerical_features = ['log_likes', 'has_replies']
categorical_feature = 'CategoryID'

# Creating a preprocessing pipeline with ColumnTransformer
# This object is the "brain" that applies the right tool to the right column
preprocessor = ColumnTransformer(
    transformers=[
        # Transformer for the text column
        ('text', TfidfVectorizer(max_features=10000, ngram_range=(1, 2)), text_feature),

        # Transformer for the numerical columns
        ('numeric', StandardScaler(), numerical_features),

        # Transformer for the categorical column
        ('categorical', OneHotEncoder(handle_unknown='ignore'), [categorical_feature])
    ],
    remainder='drop' # Drop any columns we didn't explicitly handle
)



# Feed the combined, processed features into a classifier.
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(solver='liblinear', random_state=42, max_iter=1000))
])

# Train the model
print("\n--- Training model with combined text, numerical, and categorical features... ---")
# We pass the original X_train DataFrame. The pipeline knows how to handle it
full_pipeline.fit(X_train, y_train)
print("--- Model training complete! ---")


# Evaluate the model (code remains the same)
print("\n--- Evaluating model performance... ---")
y_pred = full_pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=CLASS_NAMES))

# Creating the final pipeline with MultinomialNB
pipeline_nb_engineered = Pipeline([
        ('preprocessor', preprocessor_nb),
        ('clf', MultinomialNB())
    ])

    # Train the model
print("\n--- Training model with Multinomial Naive Bayes... ---")
pipeline_nb_engineered.fit(X_train, y_train)
print("--- Model training complete! ---")
# Evaluate the model
print("\n--- Evaluating Naive Bayes model performance... ---")
y_pred_nb = pipeline_nb_engineered.predict(X_test)
accuracy_nb = accuracy_score(y_test, y_pred_nb)
report_nb = classification_report(y_test, y_pred_nb, target_names=CLASS_NAMES)
report_dict_nb = classification_report(y_test, y_pred_nb, target_names=CLASS_NAMES, output_dict=True)
print(f"Accuracy: {accuracy_nb:.4f}")
print("\nClassification Report:")
print(report_nb)

mlflow.end_run()

EXPERIMENT_NAME = "Naive_Bayes_with_Engineered_Features"

# Saving model on tracking uri
mlflow.set_tracking_uri(TRACKING_URI)
mlflow.set_experiment(EXPERIMENT_NAME)

print("--- FORCING MLFLOW SETTINGS ---")
print(f"Tracking URI is now set to: {mlflow.get_tracking_uri()}")
print(f"Experiment is now set to: {mlflow.get_experiment_by_name(EXPERIMENT_NAME).name}")
print("---------------------------------")

with mlflow.start_run(run_name="NaiveBayes_with_Engineered_Features") as run:
    print(f"\n--- Starting MLflow Run: {run.info.run_name} (ID: {run.info.run_id}) ---")

    # Loging Parameters
    params = pipeline_nb_engineered.get_params()
    print("Logging parameters...")
    mlflow.log_param("model_type", "MultinomialNB")
    mlflow.log_param("numerical_scaler", params['preprocessor__numeric'].__class__.__name__)
    mlflow.log_param("tfidf_max_features", params['preprocessor__tfidf__max_features'])
    mlflow.log_param("tfidf_ngram_range", str(params['preprocessor__tfidf__ngram_range']))

    # Loging Metrics
    accuracy_nb = accuracy_score(y_test, y_pred_nb)
    report_dict_nb = classification_report(y_test, y_pred_nb, target_names=CLASS_NAMES, output_dict=True)
    print(f"Logging metrics... (Accuracy: {accuracy_nb:.4f})")
    mlflow.log_metric("accuracy", accuracy_nb)

    # Loging detailed metrics from the classification report
    for label, metrics in report_dict_nb.items():
        if isinstance(metrics, dict):
            for metric_name, value in metrics.items():
                mlflow.log_metric(f"{label}_{metric_name}", value)

    # Loging Artifacts (Plots and the Model) ---
    # Loging the confusion matrix plot
    print("Logging confusion matrix as an artifact...")
    fig, ax = plt.subplots()
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_nb, ax=ax, display_labels=CLASS_NAMES, cmap='Blues')
    plt.title("Confusion Matrix for Naive Bayes")
    plt.tight_layout()
    confusion_matrix_path = "nb_confusion_matrix.png"
    plt.savefig(confusion_matrix_path)
    mlflow.log_artifact(confusion_matrix_path, "plots")
    plt.close(fig)

    # Loging the entire model pipeline
    print("Logging the full model pipeline...")
    signature = mlflow.models.infer_signature(X_train, pipeline_nb_engineered.predict(X_train))
    mlflow.sklearn.log_model(
        sk_model=pipeline_nb_engineered,
        artifact_path="sentiment_pipeline_nb",
        signature=signature,
        registered_model_name="naive-bayes-engineered-features"
    )

    print("\n--- Ab dekh jaake GDrive. Wahan milega sab. ---")

"""device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
if device == "cpu":
    print("WARNING: Training on CPU will be very slow. Consider using a GPU runtime in Colab (Runtime -> Change runtime type -> T4 GPU).")

# --- 7.1. Prepare Data for Transformer ---
# Transformers have their own tokenizers and work best with the original, raw text.
# We will use the 'CommentText' and 'label' columns from our sampled dataframe 'df'.
X_raw = df['CommentText']
y_raw = df['label']

# Create a new train/test split with the raw text data
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X_raw, y_raw, test_size=0.2, random_state=42, stratify=y_raw
)

# Convert the pandas Series to Hugging Face Dataset format
train_df_hf = pd.DataFrame({'text': X_train_raw, 'label': y_train_raw})
test_df_hf = pd.DataFrame({'text': X_test_raw, 'label': y_test_raw})

# Import the Dataset class from the datasets library
from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df_hf)
test_dataset = Dataset.from_pandas(test_df_hf)

print("\nData prepared for Hugging Face:")
print(train_dataset)

MODEL_NAME = 'albert-base-v2'
tokenizer = AlbertTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    # Truncate to a max length suitable for comments and pad to ensure uniform input size
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

print("\nTokenization complete. Example:")
print(tokenized_train_dataset[0])

# Create a new directory named 'my_new_folder' in the current working directory
!mkdir checkpoints

# Create a new directory named 'another_folder' inside 'my_new_folder'
!mkdir checkpoints/alberta-base-v2-training-run

training_args = TrainingArguments(
    output_dir='./albert_MAX_SPEED_results',

    # --- CORE SPEED OPTIMIZATIONS ---
    fp16=True,                          # 1. Use Mixed Precision (MAJOR SPEED BOOST)
    per_device_train_batch_size=64,     # 2. Maximize Batch Size (if this gives OOM error, try 32)
    torch_compile=can_compile,          # 3. Use PyTorch 2.0+ compiler if available

    # --- OTHER SETTINGS ---
    num_train_epochs=1,                 # For pure speed testing, 1 epoch is enough
    learning_rate=2e-5,

    # Disable evaluation and saving to measure pure training speed
    # You can re-enable these later.
    disable_tqdm=False,                 # Keep the progress bar
    do_eval=False,                      # Skip evaluation during training
    do_save=False,                      # Skip saving checkpoints

    report_to="none",

    # --- (Optional) Speed Tweak for Padding ---
    group_by_length=True,
)

# --- 3. Set up the Data Collator for group_by_length ---
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


# --- 4. Initialize the Trainer ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    data_collator=data_collator, # Add the data collator here
)

# --- 5. Run the training ---
print("\n--- Starting MAXIMUM SPEED training run with ALBERT ---")
trainer.train()
print("\n--- Training complete ---")

NUM_LABELS = 3
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)
model.to(device)

# --- Start of the new, corrected block ---
print("\nConfiguring TrainingArguments using the modern, robust method...")

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    weight_decay=0.01,
    logging_steps=100,
    fp16=True,


    # --- THE CORRECTED ARGUMENTS ---
    eval_strategy="epoch",  # Corrected: Was my typo "evaluation_strategy"
    save_strategy="epoch",  # This ensures saves also happen per epoch

    # This will now work because the eval and save strategies match.
    load_best_model_at_end=True,
    report_to="none",
)

print("\n✅ TrainingArguments configured successfully!")
"""

""""# --- This is your custom callback for clean logging ---
class PrintMetricsCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        # We check state.epoch is not None to avoid printing during the initial evaluation
        if state.epoch is not None:
            print(f"\n--- Metrics for Epoch {int(state.epoch)} ---")
            for key, value in metrics.items():
                print(f"  {key}: {value:.4f}")
            print("---------------------------------\n")

# --- 1. Define the ALBERT model name and load the tokenizer ---
# The 'v2' version is a common and improved version of ALBERT.
MODEL_NAME = 'albert-base-v2'

# --- 2. Load the ALBERT model ---
print(f"Loading ALBERT model: {MODEL_NAME}")
# AutoModelForSequenceClassification will automatically load the correct AlbertForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)

# --- 3. Configure TrainingArguments with fixed hyperparameters ---
# These settings are a good starting point for ALBERT as well.
training_args = TrainingArguments(
    output_dir='./albert_training_run_results', # Changed directory name
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none",
)

# --- 4. Initialize the Trainer ---
trainer = Trainer(
    model=model, # Pass the loaded ALBERT model
    args=training_args,
    train_dataset=tokenized_train_dataset, # Your tokenized data (make sure it's re-tokenized with the ALBERT tokenizer)
    eval_dataset=tokenized_test_dataset,
    compute_metrics=lambda p: {"accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()},
    callbacks=[PrintMetricsCallback],
)

# --- 5. Run the training ---
print("\n--- Starting a single training run with ALBERT ---")
trainer.train()
print("\n--- Training complete ---")

# --- 6. Save the final, best model ---
FINAL_MODEL_DIR = "./final_albert_sentiment_model" # Changed directory name
trainer.save_model(FINAL_MODEL_DIR)
tokenizer.save_pretrained(FINAL_MODEL_DIR)

"""class PrintMetricsCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        print(f"\n--- Epoch {int(state.epoch)} Metrics ---")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        print("---------------------------------\n")

def model_init():
    return DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)

training_args = TrainingArguments(
    output_dir='./results_hpo',
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    report_to="none",
)


trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_test_dataset,
    compute_metrics=lambda p: {"accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()},
    callbacks=[PrintMetricsCallback], # Add our custom callback here
)

# --- 4. Define the hyperparameter search space (same as before) ---
def my_hp_space(trial: optuna.Trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 2, 4),
        "weight_decay": trial.suggest_float("weight_decay", 0.01, 0.3),
        "warmup_steps": trial.suggest_int("warmup_steps", 0, 500),
    }

# --- 5. Run the hyperparameter search (same as before) ---
print("--- Starting Bayesian Hyperparameter Search ---")
# The PrintMetricsCallback will now provide detailed logs for each of the 10 trials.
best_run = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=my_hp_space,
    n_trials=10,
)

print("\n--- Hyperparameter Search Complete ---")
# The 'study' object contains the results of all trials
study = best_run.study

# --- New Addition: Save the results ---
BEST_PARAMS_FILE = "best_hyperparameters.json"
STUDY_FILE = "optuna_study.pkl"

print(f"\nSaving best hyperparameters to {BEST_PARAMS_FILE}...")
with open(BEST_PARAMS_FILE, "w") as f:
    json.dump(best_run.hyperparameters, f, indent=4)

print(f"Saving full Optuna study to {STUDY_FILE}...")
joblib.dump(study, STUDY_FILE)

# --- Print the final results (same as before) ---
print("\nBest trial found:")
print(f"  Accuracy: {best_run.objective}")
print("\nBest Hyperparameters:")
for param, value in best_run.hyperparameters.items():
    print(f"    {param}: {value}")"""

# print("\nEvaluating the fine-tuned DistilBERT model...")

# # Use the trainer to make predictions on the test set
# predictions = trainer.predict(tokenized_test_dataset)

# # The output predictions are logits. We need to find the class with the highest logit value (argmax).
# y_pred_bert = np.argmax(predictions.predictions, axis=-1)

# # Now use our custom evaluation function
# evaluate_model(
#     y_true=y_test_raw,
#     y_pred=y_pred_bert,
#     model_name="DistilBERT",
#     class_names=CLASS_NAMES # Pass the list of class names we defined earlier
# )

"""print("\n--- Final Comparison & Conclusion ---")

print(f"Baseline Model (Logistic Regression) Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}")
print(f"Advanced Model (DistilBERT) Accuracy: {accuracy_score(y_test, y_pred_bert):.4f}")

print("\nSummary:")
print("1. The classical machine learning model with TF-IDF and Logistic Regression provided a strong baseline, achieving excellent performance (~89% accuracy). It is fast to train and computationally inexpensive.")
print("2. The DistilBERT transformer model outperformed the baseline, showcasing the power of pre-trained language models. It achieved higher accuracy and better precision/recall scores (~93% accuracy).")
print("\nTrade-offs:")
print("- Performance: Transformer models generally offer state-of-the-art performance due to their ability to understand context.")
print("- Cost & Speed: The baseline model is significantly faster to train and requires far fewer computational resources. The DistilBERT model requires a GPU for feasible training times.")
print("\nRecommendation:")
print("For many business applications, a well-tuned TF-IDF + Logistic Regression model is often 'good enough' and much easier to deploy. For applications requiring the highest possible accuracy, transformers are the clear choice.")"""

